{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n",
      "<dtype: 'int32'>\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Constant\n",
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.zeros( shape=(2, 3) ))\n",
    "print(tf.ones( shape=(2, 3) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.40533835 -0.38241544]\n",
      " [ 0.2789592  -0.22413269]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0 7]\n",
      " [4 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Random tensors\n",
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "\n",
    "y = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 1.2160999 , -0.46850505],\n",
      "       [ 0.7336236 , -2.635396  ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "init_val = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(init_val)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-1.4831139 , -0.91129625],\n",
      "       [ 0.7267284 ,  0.1639861 ]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.48311388,  0.08870375],\n",
      "       [ 1.7267284 ,  1.1639861 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "\n",
    "print(a)\n",
    "\n",
    "added_value = tf.ones(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.78119344  1.535048  ]\n",
      " [ 1.4116659  -0.01090778]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.13829057 0.3618697 ]\n",
      " [5.230093   0.01428382]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.4504508  1.8249563 ]\n",
      " [9.844763   0.88735074]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A = tf.random.normal(shape=(2, 2))\n",
    "B = tf.random.normal(shape=(2, 2))\n",
    "C = A + B\n",
    "print(C)\n",
    "\n",
    "C = tf.square(A)\n",
    "print(C)\n",
    "\n",
    "C = tf.exp(A)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # tape.watch(a) # recording the operations applied to a\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [[1.1067553 1.258142 ]\n",
      " [2.0347679 1.459056 ]]\n",
      "dc_da : [[-0.29737923 -0.80857784]\n",
      " [-0.9786953  -0.84093577]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Higher-order derivatives\n",
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(a)\n",
    "        c = tf.sqrt((tf.square(a) + tf.square(b)))\n",
    "        print(f\"c: {c}\")\n",
    "        dc_da = tape.gradient(c, a)\n",
    "        print(f\"dc_da : {dc_da}\")\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w @ x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        units = self.units\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : [[ 1.1149738  -0.0498697 ]\n",
      " [-0.30876592  1.2861652 ]]\n",
      "y : [[-0.03949983 -0.01233765 -0.10583743 -0.01148053]\n",
      " [ 0.07072432 -0.09807748  0.02978218 -0.065705  ]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear layer\n",
    "linear_layer = Linear(units=4)\n",
    "\n",
    "# The layer can be treated as a function\n",
    "x = tf.random.normal(shape=(2, 2))\n",
    "y = linear_layer(x)\n",
    "\n",
    "print(f\"x : {x}\")\n",
    "print(f\"y : {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data_path = \"/Users/mghifary/Work/Code/AI/data/mnist.npz\"\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\")/255., y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a linear layer with 10 units\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer\n",
    "optimizer = keras.optimizers.legacy.SGD(learning_rate=1e-3) # faster on Apple M1/M2 chip\n",
    "# optimizer = keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.382845878601074, Elapsed time: 0.01374506950378418\n",
      "Step: 100, Loss: 2.2317450046539307, Elapsed time: 0.0023469924926757812\n",
      "Step: 200, Loss: 2.181346893310547, Elapsed time: 0.0017001628875732422\n",
      "Step: 300, Loss: 2.0270278453826904, Elapsed time: 0.0017709732055664062\n",
      "Step: 400, Loss: 1.9355878829956055, Elapsed time: 0.0019321441650390625\n",
      "Step: 500, Loss: 1.957491397857666, Elapsed time: 0.0019001960754394531\n",
      "Step: 600, Loss: 1.759414792060852, Elapsed time: 0.0016651153564453125\n",
      "Step: 700, Loss: 1.7332817316055298, Elapsed time: 0.0017499923706054688\n",
      "Step: 800, Loss: 1.754225492477417, Elapsed time: 0.0019469261169433594\n",
      "Step: 900, Loss: 1.5429472923278809, Elapsed time: 0.0017049312591552734\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the batches of the dataset\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    start_t = timer.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of the loss wrt the weights..\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, linear_layer.trainable_weights)\n",
    "    )\n",
    "    elapsed_t = timer.time() - start_t\n",
    "\n",
    "    # Logging\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {float(loss)}, Elapsed time: {elapsed_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers that own layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y : [[ 0.0024752  -0.00264416 -0.01154168 -0.00092896 -0.01484032 -0.0029017\n",
      "   0.00040229  0.01672307 -0.0073487   0.00290369]\n",
      " [ 0.0024752  -0.00264416 -0.01154168 -0.00092896 -0.01484032 -0.0029017\n",
      "   0.00040229  0.01672307 -0.0073487   0.00290369]\n",
      " [ 0.0024752  -0.00264416 -0.01154168 -0.00092896 -0.01484032 -0.0029017\n",
      "   0.00040229  0.01672307 -0.0073487   0.00290369]]\n"
     ]
    }
   ],
   "source": [
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of linear layers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "model = MLP()\n",
    "\n",
    "y = model(tf.ones(shape=(3, 64)))\n",
    "print(f\"y : {y}\")\n",
    "\n",
    "\n",
    "assert len(model.weights) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking losses created by layers\n",
    "\n",
    "Layers can create losses during the forward pass via the `add_loss()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer that creates an activity sparsity regularization loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [<tf.Tensor: shape=(), dtype=float32, numpy=0.1750945>]\n",
      "y : [[-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]\n",
      " [-0.04428886 -0.02245777  0.06045137  0.02478068 -0.04105251 -0.04844959\n",
      "  -0.01363078 -0.0024152  -0.01546797  0.05749356]]\n"
     ]
    }
   ],
   "source": [
    "class SparseMLP(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "print(f\"Loss: {mlp.losses}\")\n",
    "print(f\"y : {y}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 5.947953224182129\n",
      "Step: 100, Loss: 2.599604606628418\n",
      "Step: 200, Loss: 2.421022891998291\n",
      "Step: 300, Loss: 2.3658080101013184\n",
      "Step: 400, Loss: 2.349546194076538\n",
      "Step: 500, Loss: 2.334782600402832\n",
      "Step: 600, Loss: 2.314484119415283\n",
      "Step: 700, Loss: 2.3390402793884277\n",
      "Step: 800, Loss: 2.319648504257202\n",
      "Step: 900, Loss: 2.315389394760132\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), _ = keras.datasets.mnist.load_data(data_path)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\")/255., y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.legacy.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mlp(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {float(loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 2.317469835281372\n",
      "Total running accuracy so far: 0.078125\n",
      "Epoch: 0, Step: 200, Loss: 0.3991779386997223\n",
      "Total running accuracy so far: 0.7653917670249939\n",
      "Epoch: 0, Step: 400, Loss: 0.3472175598144531\n",
      "Total running accuracy so far: 0.8283198475837708\n",
      "Epoch: 0, Step: 600, Loss: 0.213937908411026\n",
      "Total running accuracy so far: 0.8563851714134216\n",
      "Epoch: 0, Step: 800, Loss: 0.17006748914718628\n",
      "Total running accuracy so far: 0.8722495436668396\n",
      "Epoch: 1, Step: 0, Loss: 0.3476253151893616\n",
      "Total running accuracy so far: 0.875\n",
      "Epoch: 1, Step: 200, Loss: 0.13529257476329803\n",
      "Total running accuracy so far: 0.9399875402450562\n",
      "Epoch: 1, Step: 400, Loss: 0.22766488790512085\n",
      "Total running accuracy so far: 0.9413185715675354\n",
      "Epoch: 1, Step: 600, Loss: 0.14776422083377838\n",
      "Total running accuracy so far: 0.941841721534729\n",
      "Epoch: 1, Step: 800, Loss: 0.2048586905002594\n",
      "Total running accuracy so far: 0.9422401785850525\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the model, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "            # update the state of the accuracy metric\n",
    "            accuracy.update_state(y, logits)\n",
    "\n",
    "            # update the weights of the model to minimize the loss value\n",
    "            gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Epoch: {epoch}, Step: {step}, Loss: {float(loss_value)}\")\n",
    "            print(f\"Total running accuracy so far: {accuracy.result()}\")\n",
    "\n",
    "    # Reset the metric's state at the end of an epoch\n",
    "    accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiled functions\n",
    "\n",
    "Running eagerly is great for debugging, but we'll get better performance by compiling the computation into static graphs. Static graphs are a researcher's best friends. We can compile any function by wrapping it in a `tf.function()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.3639283180236816\n",
      "Step: 100, Loss: 0.4514067471027374\n",
      "Step: 200, Loss: 0.3441607654094696\n",
      "Step: 300, Loss: 0.6879332065582275\n",
      "Step: 400, Loss: 0.42147642374038696\n",
      "Step: 500, Loss: 0.2743704319000244\n",
      "Step: 600, Loss: 0.23235085606575012\n",
      "Step: 700, Loss: 0.14530634880065918\n",
      "Step: 800, Loss: 0.24023517966270447\n",
      "Step: 900, Loss: 0.24263697862625122\n"
     ]
    }
   ],
   "source": [
    "# Prepare our model, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fun = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "\n",
    "@tf.function # make it faster!\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "# Prepare a dataset\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data(data_path)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\")/255., y_train)\n",
    ")\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {float(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
